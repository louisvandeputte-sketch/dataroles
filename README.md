# DataRoles

Job aggregation and enrichment platform.

## ðŸš€ Deployment

**Production:** Deployed on Railway.app
- All scraping and enrichment runs on Railway 24/7
- Automatic deploys from `main` branch
- URL: Check Railway dashboard for your app URL

## ðŸ’» Local Development

**Local = Monitoring Only (No Scraping!)**

To avoid duplicate scrapes and API costs, local development only runs the web interface for monitoring.

## Architecture

- **Database**: Supabase (PostgreSQL)
- **Backend**: Python 3.11+ with FastAPI
- **Scraping**: Bright Data LinkedIn Jobs Scraper (dataset: gd_lpfll7v5hcqtkxl6l)
- **Frontend**: HTML + Tailwind CSS + Alpine.js
- **Deployment**: Cloud-ready architecture with local development support

## Project Structure

```
dataroles/
â”œâ”€â”€ config/          # Configuration and settings
â”œâ”€â”€ database/        # Database schema and client
â”œâ”€â”€ models/          # Pydantic data models
â”œâ”€â”€ clients/         # External API clients (Bright Data)
â”œâ”€â”€ ingestion/       # Data processing and normalization
â”œâ”€â”€ scraper/         # Scrape orchestration and lifecycle
â”œâ”€â”€ web/             # FastAPI web interface
â”œâ”€â”€ utils/           # Shared utilities
â””â”€â”€ tests/           # Test suite
```

## Setup

### 1. Prerequisites

- Python 3.11 or higher
- Supabase account and project
- Bright Data account with LinkedIn Jobs Scraper access

### 2. Installation

```bash
# Clone the repository
cd dataroles

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Configuration

```bash
# Copy environment template
cp .env.example .env

# Edit .env with your credentials
# - SUPABASE_URL and SUPABASE_KEY
# - BRIGHTDATA_API_TOKEN
```

### 4. Database Setup

Execute the database schema in your Supabase SQL editor:

```bash
# Copy contents of database/schema.sql and run in Supabase SQL Editor
```

### 5. Run the Application

```bash
# Start the web interface
python -m uvicorn web.app:app --reload --host 0.0.0.0 --port 8000

# Access at http://localhost:8000
```

## Development

### Running Tests

```bash
pytest
pytest --cov=. --cov-report=html
```

### Mock Mode

For development without consuming Bright Data credits:

```bash
# In .env
USE_MOCK_API=true
```

## Features

- **Job Scraping**: Automated LinkedIn job collection via Bright Data
- **Data Normalization**: Standardized job posting format
- **Deduplication**: Intelligent duplicate detection
- **Web Interface**: Admin dashboard for managing scrape runs
- **Monitoring**: Track scrape status and job statistics
- **LLM-Ready**: Schema prepared for future AI enrichment

## License

Proprietary
