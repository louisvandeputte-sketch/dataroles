#!/usr/bin/env python3
"""
Enrich all Data jobs that don't have LLM enrichment yet.
Runs in batches with rate limiting to avoid API overload.
"""

import sys
from pathlib import Path
import time
import argparse
from datetime import datetime

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from loguru import logger
from database.client import db
from ingestion.llm_enrichment import enrich_job_with_llm, save_enrichment_to_db


def get_unenriched_data_jobs(limit: int = 1000):
    """
    Get Data jobs that haven't been enriched yet.
    
    Args:
        limit: Maximum number of jobs to fetch
        
    Returns:
        List of job dictionaries with descriptions
    """
    logger.info(f"Fetching up to {limit} unenriched Data jobs...")
    
    # First get unenriched job IDs
    result = db.client.table("llm_enrichment")\
        .select("job_posting_id")\
        .not_.is_("enrichment_completed_at", "null")\
        .execute()
    
    enriched_job_ids = [row["job_posting_id"] for row in result.data] if result.data else []
    
    # Get Data jobs that are not enriched
    query = db.client.table("job_postings")\
        .select("id, title")\
        .eq("title_classification", "Data")\
        .eq("is_active", True)
    
    # Exclude already enriched jobs
    if enriched_job_ids:
        query = query.not_.in_("id", enriched_job_ids)
    
    result = query.limit(limit).execute()
    
    jobs = result.data if result.data else []
    logger.info(f"Found {len(jobs)} unenriched Data jobs")
    
    # Get descriptions for each job
    jobs_with_descriptions = []
    for job in jobs:
        desc_result = db.client.table("job_descriptions")\
            .select("full_description_text")\
            .eq("job_posting_id", job["id"])\
            .single()\
            .execute()
        
        if desc_result.data:
            job["description"] = desc_result.data.get("full_description_text", "")
            jobs_with_descriptions.append(job)
    
    logger.info(f"Retrieved descriptions for {len(jobs_with_descriptions)} jobs")
    
    return jobs_with_descriptions


def enrich_data_jobs_batch(
    batch_size: int = 10,
    delay_seconds: int = 2,
    max_jobs: int = None,
    skip_confirmation: bool = False
):
    """
    Enrich Data jobs in batches with rate limiting.
    
    Args:
        batch_size: Number of jobs to process in each batch
        delay_seconds: Seconds to wait between each job
        max_jobs: Maximum total jobs to enrich (None = all)
        skip_confirmation: Skip user confirmation
    """
    logger.info("=" * 80)
    logger.info("ENRICHING DATA JOBS WITH LLM")
    logger.info("=" * 80)
    
    # Get unenriched jobs
    limit = max_jobs if max_jobs else 1000
    jobs = get_unenriched_data_jobs(limit=limit)
    
    if not jobs:
        logger.info("‚úÖ No unenriched Data jobs found!")
        return
    
    total_jobs = len(jobs)
    if max_jobs:
        total_jobs = min(total_jobs, max_jobs)
        jobs = jobs[:max_jobs]
    
    logger.info(f"üìä Will enrich {total_jobs} Data jobs")
    logger.info(f"‚è±Ô∏è  Delay: {delay_seconds}s between jobs")
    logger.info(f"üì¶ Batch size: {batch_size}")
    logger.info(f"‚è≥ Estimated time: ~{(total_jobs * delay_seconds) / 60:.1f} minutes")
    
    # Confirm before proceeding
    if not skip_confirmation:
        print("\n" + "=" * 80)
        print(f"‚ö†Ô∏è  This will enrich {total_jobs} jobs using OpenAI API")
        print(f"üí∞ Estimated cost: ~${total_jobs * 0.01:.2f} (rough estimate)")
        print("=" * 80)
        confirm = input("\nProceed? (yes/no): ").strip().lower()
        
        if confirm != "yes":
            logger.info("‚ùå Cancelled by user")
            return
    
    # Process jobs
    stats = {
        "total": total_jobs,
        "successful": 0,
        "failed": 0,
        "errors": []
    }
    
    logger.info("\n" + "=" * 80)
    logger.info("STARTING ENRICHMENT")
    logger.info("=" * 80 + "\n")
    
    for i, job in enumerate(jobs, 1):
        job_id = job["id"]
        title = job.get("title", "Unknown")
        description = job.get("description", "")
        
        logger.info(f"[{i}/{total_jobs}] Enriching: {title}")
        
        try:
            # Enrich with LLM
            enrichment_data, error = enrich_job_with_llm(job_id, description)
            
            if enrichment_data:
                # Save to database
                success = save_enrichment_to_db(job_id, enrichment_data)
                
                if success:
                    stats["successful"] += 1
                    logger.success(f"‚úÖ [{i}/{total_jobs}] Enriched: {title}")
                else:
                    stats["failed"] += 1
                    stats["errors"].append({
                        "job_id": job_id,
                        "title": title,
                        "error": "Failed to save to database"
                    })
                    logger.error(f"‚ùå [{i}/{total_jobs}] Failed to save: {title}")
            else:
                stats["failed"] += 1
                stats["errors"].append({
                    "job_id": job_id,
                    "title": title,
                    "error": error or "Unknown error"
                })
                logger.error(f"‚ùå [{i}/{total_jobs}] Failed: {title} - {error}")
            
            # Rate limiting
            if i < total_jobs:
                time.sleep(delay_seconds)
                
        except Exception as e:
            stats["failed"] += 1
            stats["errors"].append({
                "job_id": job_id,
                "title": title,
                "error": str(e)
            })
            logger.error(f"‚ùå [{i}/{total_jobs}] Exception: {title} - {e}")
            
            # Continue with rate limiting
            if i < total_jobs:
                time.sleep(delay_seconds)
    
    # Print summary
    logger.info("\n" + "=" * 80)
    logger.info("ENRICHMENT COMPLETE")
    logger.info("=" * 80)
    logger.info(f"‚úÖ Successful: {stats['successful']}")
    logger.info(f"‚ùå Failed: {stats['failed']}")
    logger.info(f"üìä Success rate: {(stats['successful'] / stats['total'] * 100):.1f}%")
    
    if stats["errors"]:
        logger.info("\n" + "=" * 80)
        logger.info("ERRORS")
        logger.info("=" * 80)
        for error in stats["errors"][:10]:  # Show first 10 errors
            logger.error(f"Job: {error['title']}")
            logger.error(f"Error: {error['error']}")
            logger.error("-" * 40)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Enrich Data jobs with LLM"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=10,
        help="Number of jobs per batch (default: 10)"
    )
    parser.add_argument(
        "--delay",
        type=int,
        default=2,
        help="Seconds between jobs (default: 2)"
    )
    parser.add_argument(
        "--max-jobs",
        type=int,
        default=None,
        help="Maximum jobs to enrich (default: all)"
    )
    parser.add_argument(
        "--yes", "-y",
        action="store_true",
        help="Skip confirmation prompt"
    )
    
    args = parser.parse_args()
    
    enrich_data_jobs_batch(
        batch_size=args.batch_size,
        delay_seconds=args.delay,
        max_jobs=args.max_jobs,
        skip_confirmation=args.yes
    )
